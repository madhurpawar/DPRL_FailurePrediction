# âš™ï¸ System Reliability Optimization using Dynamic Programming & Reinforcement Learning  

## ğŸš€ Project Overview  
This project models a **deteriorating system's failure probability** using **Dynamic Programming (DP) and Reinforcement Learning (RL)** to optimize **replacement and preventive maintenance decisions**. By leveraging **stationary distribution analysis, Monte Carlo simulation, Poisson's equation, and Bellmanâ€™s optimality equation**, we compute the **long-run average cost** and determine the **best maintenance policy**.

---

## ğŸ” Key Features  
âœ”ï¸ **Failure Probability Tracking** â€“ Models system deterioration over **91 states**  
âœ”ï¸ **Stationary Distribution Calculation** â€“ Determines steady-state probabilities  
âœ”ï¸ **Monte Carlo Simulation** â€“ Validates failure patterns over **100,000 iterations**  
âœ”ï¸ **Poisson Equation Solution** â€“ Computes long-run expected costs  
âœ”ï¸ **Policy & Value Iteration** â€“ Finds the **optimal preventive maintenance strategy**  

---

## ğŸ— Methods Used  
- **Markov Chains** for stationary distribution analysis  
- **Monte Carlo Simulation** for long-run system behavior verification  
- **Poissonâ€™s Equation** to calculate expected limiting cost  
- **Bellmanâ€™s Equation** to optimize preventive replacement decisions  
- **Policy Iteration & Value Iteration** for strategic cost reduction  

---

## ğŸ“Š Results & Insights  
- **Long-run average cost (stationary distribution):** `0.1461`  
- **Monte Carlo simulation verified cost:** `0.1439`  
- **Poisson equation solution:** `0.1461`  
- **Optimal maintenance policy reduces cost to:** `0.1449`  

These results consistently demonstrate the effectiveness of the model in reducing maintenance costs.

---

## ğŸ“· Visualizations  
ğŸ”¹ **Failure Probability Evolution** â€“ Visualizes system degradation over time  
ğŸ”¹ **Monte Carlo Distribution** â€“ Compares theoretical vs simulated stationary behavior  
ğŸ”¹ **Optimal Replacement Policy** â€“ Highlights preventive vs reactive strategies  